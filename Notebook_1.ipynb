{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kEkgREkcCd3"
      },
      "source": [
        "# Task-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW-Ed3zRYYmi"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0M5jXdiYhQz"
      },
      "source": [
        "## Making the file of domain wise dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKhanuAKYeG1"
      },
      "outputs": [],
      "source": [
        "# Define the column names you want to keep in the new dataset\n",
        "columns_to_keep = ['YEAR','TRACTFIPS','COUNTYFIPS','STATEFIPS','STATE','COUNTY','REGION','TERRITORY']\n",
        "\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "Geographic_df = df[columns_to_keep].copy()\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(Geographic_df)# Define the column names you want to keep in the new dataset\n",
        "columns_to_keep = ['ACS_PCT_HH_SMARTPHONE_ONLY','ACS_PCT_HH_OTHER_COMP','ACS_PCT_LT_HS','ACS_PER_CAPITA_INC', 'ACS_MEDIAN_RENT','ACS_PCT_VET_COLLEGE','ACS_PCT_OTHER_INS','ACS_PCT_FINANCE','ACS_PCT_ADMIN','ACS_PCT_TRANSPORT','ACS_PCT_PROFESS','ACS_PCT_ART','ACS_PCT_MANUFACT','ACS_PCT_CTZ_NATURALIZED','ACS_PCT_CHILD_DISAB','ACS_PCT_BLACK']\n",
        "\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "Socioeconomic_df = df[columns_to_keep].copy()\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(Socioeconomic_df)# Define the column names you want to keep in the new dataset\n",
        "columns_to_keep = ['ACS_PCT_AGE_15_17','ACS_PCT_MARRIED_SP_AB_M','ACS_PCT_MARRIED_SP_AB_F','ACS_PCT_OTH_EURP','ACS_PCT_OTH_EURP', 'ACS_TOT_POP_16_19','ACS_PCT_MULT_RACE','ACS_PCT_AGE_0_17','ACS_PCT_AGE_30_44','ACS_PCT_AGE_45_64','ACS_PCT_AGE_ABOVE65','ACS_PCT_MALE','ACS_PCT_VET'\n",
        "]\n",
        "\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "Demographic_df = df[columns_to_keep].copy()\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(Demographic_df)# Define the column names you want to keep in the new dataset\n",
        "columns_to_keep = ['POS_DIST_CLINIC_TRACT','POS_DIST_ED_TRACT','POS_DIST_TRAUMA_TRACT','ACS_PCT_UNINSURED','ACS_PCT_WORK_RES_F','ACS_PCT_COMMT_60MINUP','ACS_PCT_TAXICAB_2WORK']\n",
        "\n",
        "\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "Healthcare_df = df[columns_to_keep].copy()\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(Healthcare_df)# Define the column names you want to keep in the new dataset\n",
        "columns_to_keep = ['ACS_PCT_HH_BROADBAND_ONLY','ACS_PCT_HH_SAT_INTERNET','ACS_PCT_HH_INTERNET_NO_SUBS']\n",
        "\n",
        "\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "Internet_df = df[columns_to_keep].copy()\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(Internet_df)\n",
        "# Define the column names you want to keep in the new dataset\n",
        "columns_to_keep = [ 'OBESITY_Data_Value','MAMMOUSE_Data_Value','DEPRESSION_Data_Value','CHOLSCREEN_Data_Value','COLON_SCREEN_Data_Value'\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "Health_behaviour_df = df[columns_to_keep].copy()\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(Health_behaviour_df)\n",
        "# Define the column names you want to keep in the new dataset\n",
        "columns_to_keep = ['ACS_PCT_PVT_EMPL_DRCT','ACS_PCT_WHOLESALE','ACS_PCT_HH_1PERS','ACS_PCT_CONSTRUCT','ACS_PCT_INFORM','ACS_PCT_HU_OIL','ACS_PCT_IN_COUNTY_MOVE','ACS_PCT_IN_STATE_MOVE','ACS_TOT_GRANDCHILDREN_GP']\n",
        "\n",
        "\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "Housing_df = df[columns_to_keep].copy()\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(Housing_df)\n",
        "# Define the column names you want to keep in the new dataset\n",
        "columns_to_keep = ['HIFLD_DIST_UC_TRACT','CEN_AREALAND_SQM_TRACT']\n",
        "\n",
        "\n",
        "\n",
        "# Create a new DataFrame with the selected columns\n",
        "Environmental_df = df[columns_to_keep].copy()\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(Environmental_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp5erSX5Yme6"
      },
      "source": [
        "## Using Quantile Transform on the demographic dataset for an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVChU6vGYk2O"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "scaler = QuantileTransformer(output_distribution='normal')\n",
        "df_normalized_quantile = pd.DataFrame(scaler.fit_transform(Demographic_df), columns=Demographic_df.columns)\n",
        "df2 = df['Unnamed: 0']\n",
        "df_normalized_quantile.set_index(df2, inplace=True)\n",
        "data2 = df_normalized\n",
        "correlation_matrix = data2.corr()\n",
        "low_correlation_set = set()\n",
        "high_positive_correlation_set = set()\n",
        "high_negative_correlation_set = set()\n",
        "low_correlation_set.add(0)\n",
        "for i in range(1, len(correlation_matrix.columns)):\n",
        "    correlations = correlation_matrix.iloc[i, :i]\n",
        "    print(correlations)\n",
        "    if all(correlations.abs() <= 0.75):\n",
        "        low_correlation_set.add(i)\n",
        "    elif any(correlations > 0.75):\n",
        "        high_positive_correlation_set.add(i)\n",
        "    elif any(correlations < -0.75):\n",
        "        high_negative_correlation_set.add(i)\n",
        "print(\"Columns with correlations between -0.5 and 0.5 with previous columns:\")\n",
        "print(low_correlation_set)\n",
        "print(\"\\nColumns with at least one correlation greater than 0.5 with previous columns:\")\n",
        "print(high_positive_correlation_set)\n",
        "print(\"\\nColumns with at least one correlation less than -0.5 with previous columns:\")\n",
        "print(high_negative_correlation_set)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz72X1VmYzBe"
      },
      "source": [
        "## Checking the best no. of clusters for K-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b3ndlnuYz5I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset\n",
        "# For example:\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Define the range of clusters\n",
        "cluster_range = range(2, 11)\n",
        "\n",
        "# Initialize lists to store scores\n",
        "silhouette_scores = []\n",
        "davies_bouldin_scores = []\n",
        "calinski_harabasz_scores = []\n",
        "\n",
        "# Iterate over the cluster range\n",
        "for n_clusters in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    clusters = kmeans.fit_predict(df_normalized_quantile)\n",
        "\n",
        "    # Calculate Silhouette Score\n",
        "    silhouette = silhouette_score(df_normalized_quantile, clusters)\n",
        "    silhouette_scores.append(silhouette)\n",
        "    print(f\"Silhouette Score for {n_clusters} clusters:\", silhouette)\n",
        "\n",
        "    # Calculate Davies-Bouldin Score\n",
        "    davies_bouldin = davies_bouldin_score(df_normalized_quantile, clusters)\n",
        "    davies_bouldin_scores.append(davies_bouldin)\n",
        "    print(f\"Davies-Bouldin Score for {n_clusters} clusters:\", davies_bouldin)\n",
        "\n",
        "    # Calculate Calinski-Harabasz Score\n",
        "    calinski_harabasz = calinski_harabasz_score(df_normalized_quantile, clusters)\n",
        "    calinski_harabasz_scores.append(calinski_harabasz)\n",
        "    print(f\"Calinski-Harabasz Score for {n_clusters} clusters:\", calinski_harabasz)\n",
        "\n",
        "# Elbow method for determining the optimal number of clusters\n",
        "distortion_scores = []\n",
        "for n_clusters in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(df_normalized_quantile)\n",
        "    distortion_scores.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow method\n",
        "plt.plot(cluster_range, distortion_scores, 'bx-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()\n",
        "\n",
        "# Print the scores\n",
        "print(\"Silhouette Scores:\", silhouette_scores)\n",
        "print(\"Davies-Bouldin Scores:\", davies_bouldin_scores)\n",
        "print(\"Calinski-Harabasz Scores:\", calinski_harabasz_scores)\n",
        "# Create a DataFrame for the results\n",
        "results = {'Clusters': list(cluster_range),\n",
        "           'Silhouette Score': silhouette_scores,\n",
        "           'Davies-Bouldin Score': davies_bouldin_scores,\n",
        "           'Calinski-Harabasz Score': calinski_harabasz_scores}\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTu-zBWsY3uq"
      },
      "source": [
        "## Finding the best no. of clusters for GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyMd5c6oY4Dx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset\n",
        "# For example:\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Define the range of clusters\n",
        "cluster_range = range(2, 11)\n",
        "\n",
        "# Initialize lists to store scores\n",
        "results = []\n",
        "\n",
        "# Iterate over the cluster range\n",
        "for n_clusters in cluster_range:\n",
        "    gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
        "    clusters = gmm.fit_predict(df)\n",
        "\n",
        "    # Calculate Silhouette Score\n",
        "    silhouette = silhouette_score(df, clusters)\n",
        "    print(f\"Silhouette Score for {n_clusters} clusters:\", silhouette)\n",
        "\n",
        "    # Calculate Davies-Bouldin Score\n",
        "    davies_bouldin = davies_bouldin_score(df, clusters)\n",
        "    print(f\"Davies-Bouldin Score for {n_clusters} clusters:\", davies_bouldin)\n",
        "\n",
        "    # Calculate Calinski-Harabasz Score\n",
        "    calinski_harabasz = calinski_harabasz_score(df, clusters)\n",
        "    print(f\"Calinski-Harabasz Score for {n_clusters} clusters:\", calinski_harabasz)\n",
        "\n",
        "    # Append results to the list\n",
        "    results.append([n_clusters, silhouette, davies_bouldin, calinski_harabasz])\n",
        "\n",
        "# Create a DataFrame for the results\n",
        "df_results = pd.DataFrame(results, columns=['Clusters', 'Silhouette Score', 'Davies-Bouldin Score', 'Calinski-Harabasz Score'])\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij4amcBHY84w"
      },
      "source": [
        "## Finding the best no. of clusters for DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjtdlZCPY9OH"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "min_samples_range = {10,20,30,40,50,60,70,80,90,10000}\n",
        "# Initialize lists to store scores and clusters\n",
        "results = []\n",
        "cluster_results = []\n",
        "\n",
        "# Fixed epsilon value\n",
        "eps = 0.4\n",
        "\n",
        "# Iterate over the min_samples values\n",
        "for min_samples in min_samples_range:\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    clusters = dbscan.fit_predict(df)\n",
        "\n",
        "    # Check for at least two clusters\n",
        "    if len(set(clusters)) > 1:\n",
        "        # Print clusters\n",
        "        unique_clusters = set(clusters)\n",
        "        print(f\"Clusters for min_samples={min_samples}:\")\n",
        "\n",
        "        for cluster_label in unique_clusters:\n",
        "            cluster_rows = df[clusters == cluster_label]\n",
        "            print(f\"Cluster {cluster_label}:\", cluster_rows.index.tolist())\n",
        "\n",
        "        # Calculate Silhouette Score\n",
        "        silhouette = silhouette_score(df, clusters)\n",
        "        print(f\"Silhouette Score for min_samples={min_samples}:\", silhouette)\n",
        "\n",
        "        # Calculate Davies-Bouldin Score\n",
        "        davies_bouldin = davies_bouldin_score(df, clusters)\n",
        "        print(f\"Davies-Bouldin Score for min_samples={min_samples}:\", davies_bouldin)\n",
        "\n",
        "        # Calculate Calinski-Harabasz Score\n",
        "        calinski_harabasz = calinski_harabasz_score(df, clusters)\n",
        "        print(f\"Calinski-Harabasz Score for min_samples={min_samples}:\", calinski_harabasz)\n",
        "\n",
        "        # Append results to the list\n",
        "        results.append([min_samples, silhouette, davies_bouldin, calinski_harabasz])\n",
        "        cluster_results.append(cluster_results)\n",
        "\n",
        "# Create a DataFrame for the results\n",
        "df_results = pd.DataFrame(results, columns=['Min Samples', 'Silhouette Score', 'Davies-Bouldin Score', 'Calinski-Harabasz Score'])\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb3Jvb8CZEAP"
      },
      "source": [
        "## Finding the best no. of clusters for HDBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jQIHQR4ZEPd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import hdbscan\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Initialize lists to store scores\n",
        "min_cluster_sizes_range = [10,20,30,40,50,60,70,80,90,100]\n",
        "\n",
        "# Initialize lists to store scores\n",
        "results = []\n",
        "\n",
        "# Iterate over the min_cluster_sizes values\n",
        "for min_cluster_size in min_cluster_sizes_range:\n",
        "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
        "    clusters = clusterer.fit_predict(df)\n",
        "\n",
        "    # Check for at least two clusters\n",
        "    if len(set(clusters)) > 1:\n",
        "        # Calculate Silhouette Score\n",
        "        silhouette = silhouette_score(df, clusters)\n",
        "        print(f\"Silhouette Score for min_cluster_size={min_cluster_size}:\", silhouette)\n",
        "\n",
        "        # Calculate Davies-Bouldin Score\n",
        "        davies_bouldin = davies_bouldin_score(df, clusters)\n",
        "        print(f\"Davies-Bouldin Score for min_cluster_size={min_cluster_size}:\", davies_bouldin)\n",
        "\n",
        "        # Calculate Calinski-Harabasz Score\n",
        "        calinski_harabasz = calinski_harabasz_score(df, clusters)\n",
        "        print(f\"Calinski-Harabasz Score for min_cluster_size={min_cluster_size}:\", calinski_harabasz)\n",
        "\n",
        "        # Print clusters\n",
        "        cluster_dict = {}\n",
        "        for i, cluster in enumerate(clusters):\n",
        "            if cluster not in cluster_dict:\n",
        "                cluster_dict[cluster] = []\n",
        "            cluster_dict[cluster].append(i)\n",
        "\n",
        "        for cluster, rows in cluster_dict.items():\n",
        "            print(f\"Cluster {cluster}: {rows}\")\n",
        "\n",
        "        # Append results to the list\n",
        "        results.append([min_cluster_size, silhouette, davies_bouldin, calinski_harabasz])\n",
        "\n",
        "# Create a DataFrame for the results\n",
        "df_results = pd.DataFrame(results, columns=['Min Cluster Size', 'Silhouette Score', 'Davies-Bouldin Score', 'Calinski-Harabasz Score'])\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3ShmE5kZLH5"
      },
      "outputs": [],
      "source": [
        "#As k-means is the best algorithm for this and the best no. of clusters are 4 making this for one demographic only and this can be done similarily for the other file by changing the file name\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "\n",
        "# Perform KMeans clustering with 3 clusters\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "clusters = kmeans.fit_predict(df_normalized_quantile)\n",
        "\n",
        "# Create a dictionary to store cluster indices and corresponding row names\n",
        "cluster_dict = {}\n",
        "for cluster_num in range(4):\n",
        "    cluster_indices = [i for i, c in enumerate(clusters) if c == cluster_num]\n",
        "    cluster_names = [df_normalized_quantile.index[idx] for idx in cluster_indices]\n",
        "    cluster_dict[cluster_num] = cluster_names\n",
        "\n",
        "# Save each cluster as a separate CSV file\n",
        "for cluster_num, row_names in cluster_dict.items():\n",
        "    cluster_df = df_normalized_quantile.loc[row_names]\n",
        "    cluster_df.to_csv(f'cluster_{cluster_num}_Demographic.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDAKxLEZNlj"
      },
      "source": [
        "#### Giving unclustered file so that their probability for being in one cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gHC40l9ZRnw"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/Unclustered_Demographic.csv',index_col = 'Unnamed: 0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uBKYAlEZZKs"
      },
      "source": [
        "#### Quantile transform of unclustered dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfrsgaHNZTJB"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "# Assuming df is your DataFrame containing the dataset\n",
        "\n",
        "# Quantile transformation\n",
        "scaler = QuantileTransformer(output_distribution='normal')\n",
        "df_normalized_quantile = pd.DataFrame(scaler.fit_transform(df_normalized_quantile), columns=df_normalized_quantile.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu1jWe6LZgDW"
      },
      "source": [
        "#### Finding Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5q2YHK8Zd15"
      },
      "outputs": [],
      "source": [
        "# Predict the cluster labels for the test dataset\n",
        "import numpy as np\n",
        "test_clusters = kmeans.predict(df_normalized_quantile)\n",
        "\n",
        "# Calculate the distances of each sample in the test dataset from the cluster centroids\n",
        "distances = kmeans.transform(df_normalized_quantile)\n",
        "\n",
        "# Calculate the probabilities of each sample belonging to each cluster\n",
        "probabilities = np.exp(-distances) / np.exp(-distances).sum(axis=1, keepdims=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRGAjqE1ZhQP"
      },
      "outputs": [],
      "source": [
        "probs  = pd.DataFrame(probabilities)\n",
        "probs.set_index(df2.index, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChhX46VbP8Vt"
      },
      "source": [
        "# Ensembling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyc1SDTX466l"
      },
      "source": [
        "## Installing Dependancies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O882tSUqpoWU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN_READ')\n",
        "%run \"(filepath for imputing_fxn.ipynb)\"\n",
        "!pip uninstall -y bitsandbytes peft trl accelerate datasets transformers\n",
        "!pip3 install -q -U bitsandbytes==0.42.0\n",
        "!pip3 install -q -U peft==0.8.2\n",
        "!pip3 install -q -U trl==0.7.10\n",
        "!pip3 install -q -U accelerate==0.27.1\n",
        "!pip3 install -q -U datasets==2.17.0\n",
        "!pip3 install -q -U transformers==4.38.0\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer, pipeline\n",
        "import json\n",
        "from peft import PeftModel\n",
        "import pandas as pd\n",
        "df2=pd.read_csv(\"filepath for Soical.csv\")\n",
        "!pip3 install -q -U Flask\n",
        "!pip3 install -q -U pyngrok\n",
        "!pip3 install flask-cors\n",
        "import heapq\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer, pipeline\n",
        "import json\n",
        "from peft import PeftModel\n",
        "import pandas as pd\n",
        "import requests\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58nafG5jOOGL"
      },
      "source": [
        "## Pre-Processing the Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C38h2-AO9yRG"
      },
      "outputs": [],
      "source": [
        "    python_data = pd.DataFrame([json_string])\n",
        "    python_data = python_data.rename(columns={'ZIP CODE': 'ZIPCODE'})\n",
        "    python_data = python_data.replace(np.nan, None)\n",
        "    python_data['ZIPCODE'][0]\n",
        "    ip_dict = {'zipcode':None,'Address':None,'Gender':None,'age':None,'Race':None,'Income':None,'Education':None,'Veteran Status':None}\n",
        "    ip_dict['zipcode']=\tpython_data['ZIPCODE'][0]\n",
        "    ip_dict['Gender']=python_data['GENDER'][0]\n",
        "    ip_dict['age']=python_data['AGE'][0]\n",
        "    ip_dict['Race']=python_data['RACE'][0]\n",
        "    ip_dict['Income']=python_data['INCOME'][0]\n",
        "    ip_dict['Education']=python_data['EDUCATION'][0]\n",
        "    ip_dict['Veteran Status']=python_data['VETERAN STATUS'][0]\n",
        "    output,address,state,county=main(ip_dict,top_60_zip,ord_cencus_all)\n",
        "    out_df=make_df(output,address,state,county)\n",
        "    out_df['ADDRESS']=python_data['ADDRESS']\n",
        "    code_to_description = dict(zip(df2['name'], df2['label']))\n",
        "    mapping = {col: code_to_description.get(col, col) for col in df2.columns}\n",
        "    out_df.rename(columns=mapping, inplace=True)\n",
        "    out_df.rename(columns=code_to_description, inplace=True)\n",
        "    def combine_column_and_value(row):\n",
        "        \"\"\" Combines a row's column names and values into a single string \"\"\"\n",
        "        result = \" \".join([f\"{col}: {str(value)}\" for col, value in row.iteritems()])\n",
        "        return result\n",
        "    new_data = out_df.apply(combine_column_and_value, axis=1).tolist()\n",
        "    new_df = pd.DataFrame(new_data, columns=['combined'])\n",
        "    def preprocessing(new_df, column_name='combined'):\n",
        "        data = new_df[column_name]\n",
        "        result = data.str.split(\"Problem 1:\", expand=True)\n",
        "        result_df = pd.DataFrame()\n",
        "        result_df['narrative'] = result[0]\n",
        "        result_df['narrative'] = result_df['narrative'] + \"You are a smart model, state the 3 most probable problems this person may face based on the information provided and also give solutions to the respective problems?(List exactly 3 problems and their respective solutions)(Try to give the output in this form:- {output: [{'problem1': 'string', 'solution1': 'string'}, {'problem2': 'string', 'solution2': 'string'}, {'problem3': 'string', 'solution3': 'string'}]})\"\n",
        "        return result_df\n",
        "    gemma_df = preprocessing(new_df.copy())\n",
        "    from datasets import Dataset\n",
        "    dataset = Dataset.from_pandas(gemma_df[[\"narrative\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr1w0b-oPXVk"
      },
      "source": [
        "## Models and thier Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfKAAJtvc44z"
      },
      "source": [
        "### Gemma-2B(instruct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyndGSx4aTvC"
      },
      "outputs": [],
      "source": [
        "model_gemma = AutoModelForCausalLM.from_pretrained(\"model path\",token=os.environ['HF_TOKEN']).to('cuda')\n",
        "tokenizer_gemma = AutoTokenizer.from_pretrained(\"model path\",token=os.environ['HF_TOKEN'])\n",
        "inputs = tokenizer_gemma(\"<start_of_turn>user\"+dataset['narrative'][0]+\"<start_of_turn>model\", return_tensors=\"pt\").to('cuda')\n",
        "outputs = model_gemma.generate(**inputs, max_new_tokens=300)\n",
        "gemma_output=tokenizer_gemma.decode(outputs[0], skip_special_tokens=True)\n",
        "split_index = gemma_output.find(')model')\n",
        "gemma_result = gemma_output[split_index + len(')model'):] \n",
        "gemma_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjbmCeS8dBOq"
      },
      "source": [
        "### Llama2-7B-chat-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX7A_Sz0GSnJ"
      },
      "outputs": [],
      "source": [
        "model_llama=AutoModelForCausalLM.from_pretrained(\"model path\").to('cuda')\n",
        "tokenizer_llama=AutoTokenizer.from_pretrained(\"model path\")\n",
        "prompt=dataset['narrative'][0]\n",
        "pipe = pipeline(task=\"text-generation\", model=model_llama, tokenizer=tokenizer_llama, max_length=10000)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "llama_output=result[0]['generated_text']\n",
        "llama_output.replace(\"<s>[INST]\",\"\")\n",
        "llama_output.replace(\"[/INST]\",\"\")\n",
        "llama_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLiA1X_-dGdm"
      },
      "source": [
        "### Mistral-7B(instruct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "dd9aaf8d6dc64d25a459d34350881b43",
            "707e88edb4fa45018c3fe9fcddc92112",
            "b27a5cdffbfc4878bf44dc4619b1180d",
            "595a860b512140b48dd14c4764ebdcc2",
            "a2758bf6e0724c6b8c489feb0bc36058",
            "3721d6ee39154015a9d41c77fa40ccad",
            "448a3c9504244000b4c05b8912d4265e",
            "40aa9f3bdad341139ef4d0fe7eba187b",
            "8c7cbd57dc814c72b775745aba71b6e9",
            "28ad7bfa99c9473d8ad75edf3ab6696f",
            "ae67d2a3505d46eeae274bbc95c76645"
          ]
        },
        "id": "SnyfoPCmTWVX",
        "outputId": "d2ca9736-b44a-4eff-ad75-fb170bfbc0b9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd9aaf8d6dc64d25a459d34350881b43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"model path\")\n",
        "eval_prompt = dataset['narrative'][0]\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\")\n",
        "\n",
        "ft_model.eval()\n",
        "with torch.no_grad():\n",
        "    mistral_output=eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=10000)[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDk31cFTgBwh"
      },
      "source": [
        "## Using Gemma-7B(instruct) to get a combined problem solution set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "qle-QQNGEd4q"
      },
      "outputs": [],
      "source": [
        "combined_output= gemma_result + llama_output+ mistral_output\n",
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/google/gemma-2b-it\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": \"I am giving you two combined passages which have three sets of problems and solutions for each passage obtained from two different models . Combine these six sets of problems and create three sets of problems and solutions.(Only combine the problems and solutions. Ignore any other unncesary text.)\" + combined_output,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6DJagYOcE_S"
      },
      "source": [
        "# Task-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9qju7LZTaVP"
      },
      "source": [
        "# Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKp7f3orUj3l"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "1.   Run the below cell to run the server.\n",
        "2.   The input is recieved from the server.\n",
        "3.   The input is pre-processed and then sent to the model.\n",
        "4.   Model's output is then fed to a fine-tuned BERT model then, it is  used to map the realtime solutions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   *The model may give an error if the input zipcode is is not from the available scraped files*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pdd__DkTWnf"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "from flask_cors import CORS\n",
        "\n",
        "# Set up Flask app\n",
        "from flask import Flask, request, jsonify\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    json_string = request.get_json()\n",
        "    print(json_string)  # Assuming JSON input\n",
        "    # Assuming your_model_module has a predict function\n",
        "\n",
        "    python_data=pd.DataFrame(json_string,index=[0])\n",
        "    python_data = python_data.rename(columns={'ZIP CODE': 'ZIPCODE'})\n",
        "    python_data = python_data.replace(np.nan, None)\n",
        "    ip_dict = {'zipcode':None,'Address':None,'Gender':None,'age':None,'Race':None,'Income':None,'Education':None,'Veteran Status':None}\n",
        "    ip_dict['zipcode']=\tpython_data['ZIPCODE'][0]\n",
        "    #print(python_data['ZIPCODE'][0])\n",
        "    #ip_dict['Address']=python_data['ADDRESS'][0]\n",
        "    ip_dict['Gender']=python_data['GENDER'][0]\n",
        "    ip_dict['age']=python_data['AGE'][0]\n",
        "    ip_dict['Race']=python_data['RACE'][0]\n",
        "    ip_dict['Income']=python_data['INCOME'][0]\n",
        "    ip_dict['Education']=python_data['EDUCATION'][0]\n",
        "    ip_dict['Veteran Status']=python_data['VETERAN STATUS'][0]\n",
        "    #print(ip_dict,top_60_zip,ord_cencus_all)\n",
        "    output,address,state,county=main(ip_dict,top_60_zip,ord_cencus_all)\n",
        "    out_df=make_df(output,address,state,county)\n",
        "    out_df['ADDRESS']=python_data['ADDRESS']\n",
        "    df2=pd.read_csv(\"soical.csv path\")\n",
        "    code_to_description = dict(zip(df2['name'], df2['label']))\n",
        "    mapping = {col: code_to_description.get(col, col) for col in df2.columns}\n",
        "    # Rename columns while preserving order\n",
        "    out_df.rename(columns=mapping, inplace=True)\n",
        "    # Rename columns in df2\n",
        "    out_df.rename(columns=code_to_description, inplace=True)\n",
        "    def combine_column_and_value(row):\n",
        "        result = \" \".join([f\"{col}: {str(value)}\" for col, value in row.items()])\n",
        "        return result\n",
        "    new_data = out_df.apply(combine_column_and_value, axis=1).tolist()\n",
        "    new_df = pd.DataFrame(new_data, columns=['combined'])\n",
        "\n",
        "    def preprocessing(new_df, column_name='combined'):\n",
        "        data = new_df[column_name]\n",
        "        result = data.str.split(\"Problem 1:\", expand=True)\n",
        "        result_df = pd.DataFrame()\n",
        "        result_df['narrative'] = result[0]\n",
        "        result_df['narrative'] = result_df['narrative'] + 'You are a smart model, state the 3 most probable problems this person may face based on the information provided and also give solutions to the respective problems?(List exactly 3 problems and their respective solutions)(Try to give the output in json string form for example:- {output: [{\"problem\": \"string\", \"solution\": \"string\"}, {\"problem\": \"string\", \"solution\": \"string\"}, {\"problem\": \"string\", \"solution\": \"string\"}]})'\n",
        "        return result_df\n",
        "    gemma_df = preprocessing(new_df.copy())\n",
        "    inputs = tokenizer_gemma(\"<start_of_turn>user\"+gemma_df['narrative'][0]+\"<start_of_turn>model\", return_tensors=\"pt\").to('/gpu:1')\n",
        "    outputs = model_gemma.generate(**inputs, max_new_tokens=300)\n",
        "    gemma_output=tokenizer_gemma.decode(outputs[0], skip_special_tokens=True)\n",
        "    split_index = gemma_output.find(')model')\n",
        "    gemma_out = gemma_output[split_index + len(')model'):]\n",
        "    gemma_out=gemma_out.replace(\"\\n\",\"\")\n",
        "\n",
        "    def get_info():\n",
        "        dict = json_string\n",
        "        return dict\n",
        "\n",
        "    def select_csv(dict):\n",
        "        try:\n",
        "            problem = dict[\"PROBLEM\"].strip()\n",
        "            zipcode = dict[\"ZIP CODE\"]\n",
        "\n",
        "            if problem == \"Age\":\n",
        "                path = \"independent-living/Independent Living/Independent Living\" + str(zipcode) + \".csv\"\n",
        "                return path\n",
        "\n",
        "            elif problem == \"Age and Income\":\n",
        "                path = \"For 164 Zips/Area Agency on Aging/Area Agency on Aging\" + str(zipcode) + \".csv\"\n",
        "                return path\n",
        "\n",
        "            elif problem == \"Disability\":\n",
        "                path = \"For 164 Zips/adult_day_care_new_name/Adult Day Care\" + str(zipcode) + \".csv\"\n",
        "                return path\n",
        "\n",
        "            elif problem == \"Transport\":\n",
        "                path = \"For 164 Zips/Transportation/Transportation\" + str(zipcode) + \".csv\"\n",
        "                return path\n",
        "\n",
        "            elif problem == \"Health\":\n",
        "                path = \"hospital/hospital/Hospital\" + str(zipcode) + \".csv\"\n",
        "                return path\n",
        "            else:\n",
        "                print(\"\\nProblem not defined\")\n",
        "                exit()\n",
        "        except FileNotFoundError:\n",
        "            print(\"File not found error occurred. We have not done for this zipcode.\\n\")\n",
        "\n",
        "\n",
        "    def get_distance_free(origin, destination):\n",
        "        url = f\"http://router.project-osrm.org/route/v1/driving/{origin[1]},{origin[0]};{destination[1]},{destination[0]}?annotations=distance\"\n",
        "\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'routes' in data and len(data['routes']) > 0:\n",
        "            distance = data['routes'][0]['distance']\n",
        "            return distance / 1000\n",
        "        else:\n",
        "            return \"Error calculating distance from open source api\"\n",
        "\n",
        "    def get_distance_google(origin, destination, api_key):\n",
        "        url = f\"https://maps.googleapis.com/maps/api/distancematrix/json?origins={origin}&destinations={destination}&key={api_key}\"\n",
        "\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        if data['status'] == 'OK':\n",
        "            distance = data['rows'][0]['elements'][0]['distance']['text']\n",
        "            return distance\n",
        "        else:\n",
        "            return \"Error calculating distance from google map api\"\n",
        "\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "\n",
        "        dict=get_info()\n",
        "        zipcode=dict[\"ZIP CODE\"]\n",
        "        address=dict[\"ADDRESS\"]\n",
        "        addr=\"address\"\n",
        "        required=None\n",
        "        distt=None\n",
        "        path=select_csv(dict)\n",
        "        data=pd.read_csv(path)\n",
        "        length=len(data)\n",
        "        if(length==0):\n",
        "            print(\"No facilities in the area\\n\")\n",
        "            exit()\n",
        "        if address == None:\n",
        "            if(dict[\"PROBLEM\"]==\"Health\"):\n",
        "                if(len!=0):\n",
        "                    required=data.iloc[0,:]\n",
        "                    distt=None\n",
        "                else:\n",
        "                    print(\"No Hospital in the ZIPCODE\")\n",
        "\n",
        "            else:\n",
        "\n",
        "             for i in range(length):\n",
        "                dis=\"distance\"\n",
        "                temp=1000000.\n",
        "                data_i=data.iloc[i,:]\n",
        "                #print(data_i[dis])\n",
        "                if(data_i[dis]< temp):\n",
        "                    temp=np.array(data[dis])[i]\n",
        "                    required=data_i\n",
        "                else:\n",
        "                    continue\n",
        "                distt=temp\n",
        "\n",
        "\n",
        "        else:\n",
        "          for i in range(length):\n",
        "                temp=10000.0\n",
        "                data_i=data.iloc[i,:]\n",
        "                destination=data_i[addr]\n",
        "                print(address,destination)\n",
        "                distance_bw=get_distance_free(address, destination)\n",
        "                print(distance_bw)\n",
        "                if(distance_bw==\"Error calculating distance from open source api\"):\n",
        "                    print(\"Problem in using free api to calaculate distance\")\n",
        "                    if(required==None):\n",
        "                        required=data_i\n",
        "                        break\n",
        "                    else:\n",
        "                        break\n",
        "                if(distance_bw < temp):\n",
        "                    temp=distance_bw\n",
        "                    required=data_i\n",
        "\n",
        "                else:\n",
        "                    continue\n",
        "        t=required.T\n",
        "        file = t.to_json(orient='records')\n",
        "\n",
        "    return jsonify({\"result\": result, \"file\": file})\n",
        "ngrok.set_auth_token('ngrok api')\n",
        "public_url = ngrok.connect()\n",
        "print(\" * Ngrok tunnel:\", public_url)\n",
        "# Run Flask app\n",
        "app.run(host='0.0.0.0', port=80)\n",
        "# Running this cell will run the server on 127.0.0.1:80 and expose port 80."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go1QNrYaWy0Q"
      },
      "source": [
        "# Scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shSk1HqMW-Y1"
      },
      "source": [
        "## Importing files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orSb-RQxTXm3"
      },
      "outputs": [],
      "source": [
        "hope=pd.read_csv('file path for final_hopefully.csv')\n",
        "zip_census=pd.read_csv('file path for zipcode_census.csv')\n",
        "df=pd.read_csv('file path for census_data.csv', encoding='ISO-8859-1')\n",
        "hope['ACS_PCT_POP_16_19'] = (df['ACS_TOT_POP_16_19']/df['ACS_TOT_POP_WT'])*100\n",
        "hope = hope.fillna({'ACS_PCT_POP_16_19': 0})\n",
        "front_pd=hope.iloc[:,[2,3,4,5,6,7,8,9]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKkuGSb_XwSZ"
      },
      "source": [
        "## The Scoring Funtion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHkj-eeeWyEe"
      },
      "outputs": [],
      "source": [
        "Socioeconomic_Factors={\n",
        "    'ACS_PCT_HH_SMARTPHONE_ONLY':[False,8]\n",
        "   ,'ACS_PCT_HH_OTHER_COMP':[True,70]\n",
        "   , 'ACS_PCT_VET_COLLEGE':[True,39]\n",
        "   , 'ACS_PCT_OTHER_INS':[True,20]\n",
        "   , 'ACS_PCT_FINANCE':[True,10,20]\n",
        "   , 'ACS_PCT_ADMIN':[True,5,15]\n",
        "   , 'ACS_PCT_TRANSPORT':[True,10,15]\n",
        "   , 'ACS_PCT_PROFESS':[True,20,30]\n",
        "   , 'ACS_PCT_ART':[True,10,20]\n",
        "   , 'ACS_PCT_MANUFACT':[True,10,20]\n",
        "   , 'ACS_PCT_CTZ_NATURALIZED':[True,10]\n",
        "   , 'ACS_PCT_CHILD_DISAB':[False,10]\n",
        "   , 'ACS_PCT_BLACK':[True,10]\n",
        "}\n",
        "Demographic_Information={\n",
        "    'ACS_PCT_AGE_15_17':[True,3.9,10]\n",
        "    ,'ACS_PCT_MARRIED_SP_AB_M':[True,5,10]\n",
        "    ,'ACS_PCT_MARRIED_SP_AB_F':[False,5]\n",
        "   , 'ACS_PCT_OTH_EURP':[True,10,15]\n",
        "   , 'ACS_PCT_POP_16_19':[True,5,10]\n",
        "   , 'ACS_PCT_MULT_RACE':[True,5,10]\n",
        "}\n",
        "Healthcare_Access_Utilization={'POS_DIST_CLINIC_TRACT':[False,9.94],\n",
        "'POS_DIST_ED_TRACT':[False,9.94],\n",
        "'POS_DIST_TRAUMA_TRACT':[False,30],\n",
        "'ACS_PCT_WORK_RES_F':[True,46],\n",
        "'ACS_PCT_COMMT_60MINUP':[False,10],\n",
        "'ACS_PCT_TAXICAB_2WORK':[True,1.1]\n",
        "            }\n",
        "Internet_and_Communication_Access={\n",
        "     'ACS_PCT_HH_BROADBAND_ONLY':[True,70]\n",
        "   , 'ACS_PCT_HH_SAT_INTERNET':[False,10]\n",
        "   , 'ACS_PCT_HH_INTERNET_NO_SUBS':[False,15]\n",
        "}\n",
        "Health_Behavior_Outcomes={\n",
        "    'OBESITY_Data_Value':[False,25]\n",
        "   , 'MAMMOUSE_Data_Value':[True,75]\n",
        "   , 'DEPRESSION_Data_Value':[False,10]\n",
        "   , 'CHOLSCREEN_Data_Value':[True,70]\n",
        "   , 'COLON_SCREEN_Data_Value':[True,70]\n",
        " }\n",
        "Housing_and_Transportation={\n",
        "    'ACS_PCT_PVT_EMPL_DRCT':[True,75]\n",
        "   , 'ACS_PCT_WHOLESALE':[True,4,6]\n",
        "   , 'ACS_PCT_CONSTRUCT':[True,5,10]\n",
        "   , 'ACS_PCT_INFORM':[True,5,7]\n",
        "   , 'ACS_PCT_HU_OIL':[True,5,7]\n",
        "   , 'ACS_PCT_IN_COUNTY_MOVE':[True,10,15]\n",
        "   , 'ACS_PCT_IN_STATE_MOVE':[True,5,7]\n",
        "\n",
        "}\n",
        "Environmental_Factors={\n",
        "   'HIFLD_DIST_UC_TRACT':[False,10]\n",
        "\n",
        "}\n",
        "domain_list=[Socioeconomic_Factors,\n",
        "Demographic_Information,\n",
        "Healthcare_Access_Utilization,\n",
        "Internet_and_Communication_Access,\n",
        " Health_Behavior_Outcomes,\n",
        "Housing_and_Transportation,\n",
        "Environmental_Factors]\n",
        "def scoring_function(zip_code) :\n",
        "    filtered_rows = zip_census[zip_census['ZIP'] == zip_code]\n",
        "    census_row = filtered_rows['TRACT'].tolist()\n",
        "    score_dist = {}\n",
        "    dom_list = ['Socioeconomic_Factors',\n",
        "                'Demographic_Information',\n",
        "                'Healthcare_Access_Utilization',\n",
        "                'Internet_and_Communication_Access',\n",
        "                'Health_Behavior_Outcomes',\n",
        "                'Housing_and_Transportation',\n",
        "                'Environmental_Factors']\n",
        "\n",
        "    # for each domain\n",
        "    for d_lst, str_d in zip(domain_list, dom_list):\n",
        "        # a df for each domain\n",
        "        d_df = front_pd.copy()\n",
        "        for j in d_lst.keys():\n",
        "            d_df[j] = hope[j]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        score_domain = {}\n",
        "        value_census = []\n",
        "        value_features = {}\n",
        "        for k in range(len(census_row)):\n",
        "                row = d_df[d_df['TRACTFIPS'] == census_row[k]].iloc[:, 8:].copy()\n",
        "                row\n",
        "                value = 0\n",
        "                #every features of row\n",
        "                for i in d_lst.keys():\n",
        "                    j=1\n",
        "                    if i not in value_features:\n",
        "                        value_features[i] = []\n",
        "                    if len(d_lst[i]) == 2:\n",
        "                        if 'PCT' in i:\n",
        "                            l = (row[i].iloc[0] - d_lst[i][1]) * j\n",
        "                            l = l / 10\n",
        "                            if d_lst[i][0]:\n",
        "                                if l>=0:\n",
        "                                    value += 0\n",
        "                                    value_features[i].append(0)\n",
        "                                else:\n",
        "                                    value += l\n",
        "                                    value_features[i].append(l)\n",
        "                            else:\n",
        "                                if l<0:\n",
        "                                    value = 0\n",
        "                                    value_features[i].append(0)\n",
        "                                else :\n",
        "                                    value-=l\n",
        "                                    value_features[i].append(-l)\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            l = (row[i].iloc[0] - d_lst[i][1]) * j\n",
        "                            l = (l * 10) / (max(d_df[i]) - min(d_df[i]))\n",
        "                            if d_lst[i][0]:\n",
        "                                if l>=0:\n",
        "                                    value += 0\n",
        "                                    value_features[i].append(0)\n",
        "                                else:\n",
        "                                    value += l\n",
        "                                    value_features[i].append(l)\n",
        "                            else:\n",
        "                                if l<0:\n",
        "                                    value = 0\n",
        "                                    value_features[i].append(0)\n",
        "                                else :\n",
        "                                    value-=l\n",
        "                                    value_features[i].append(-l)\n",
        "\n",
        "\n",
        "                    elif len(d_lst[i]) == 3:\n",
        "\n",
        "                        if 'PCT' in i:\n",
        "                            if row[i].iloc[0] >= d_lst[i][1] and row[i].iloc[0] <= d_lst[i][2]:\n",
        "\n",
        "                                l = min(row[i].iloc[0] - d_lst[i][1], d_lst[i][2] - row[i].iloc[0]) * j\n",
        "                                l = abs(l / 10)\n",
        "                                value += 0\n",
        "                                value_features[i].append(0)\n",
        "                            elif row[i].iloc[0] < d_lst[i][1]:\n",
        "                                l = abs(d_lst[i][1] - row[i]) * j\n",
        "                                l = l / 10\n",
        "                                value -= l\n",
        "                                value_features[i].append(float(-l))\n",
        "                            elif row[i].iloc[0] > d_lst[i][2]:\n",
        "                                l = abs(d_lst[i][2] - row[i]) * j\n",
        "                                l = l / 10\n",
        "                                value -= l\n",
        "                                value_features[i].append(float(-l))\n",
        "                        else:\n",
        "                            if row[i].iloc[0] >= d_lst[i][1] and row[i].iloc[0] <= d_lst[i][2]:\n",
        "                                value += 0\n",
        "                                value_features[i].append(0)\n",
        "                            elif row[i].iloc[0] < d_lst[i][1]:\n",
        "                                l = abs(d_lst[i][1] - row[i].iloc[0]) * j\n",
        "                                l = (l * 10) / (max(d_df[i]) - min(d_df[i]))\n",
        "                                value -= l\n",
        "                                value_features[i].append(float(-l))\n",
        "                            elif row[i].iloc[0] > d_lst[i][2]:\n",
        "                                l = abs(d_lst[i][2] - row[i].iloc[0]) * j\n",
        "                                l = (l * 10) / (max(d_df[i]) - min(d_df[i]))\n",
        "                                value -= l\n",
        "                                value_features[i].append(float(-l))\n",
        "\n",
        "        score_features=[]\n",
        "        for i in value_features.keys():\n",
        "            non_zero_count = len([x for x in value_features[i] if x != 0])\n",
        "            if non_zero_count>1:\n",
        "                    feature = np.mean(value_features[i][value_features[i] != 0])\n",
        "            else :\n",
        "                feature=value_features[i][0]\n",
        "\n",
        "\n",
        "            score_domain[i] = feature\n",
        "            score_features.append(feature)\n",
        "        score_domain['Score']=np.sum(score_features)\n",
        "        score_dist[str_d] = score_domain\n",
        "    score_3main = {}\n",
        "\n",
        "    for i in score_dist.keys():\n",
        "        score_3main[i] = {}  # Initialize an empty dictionary for each domain\n",
        "        least_keys = heapq.nsmallest(4, score_dist[i], key=score_dist[i].get)\n",
        "        for j in least_keys:\n",
        "            if j != 'Score':\n",
        "                score_3main[i][j] = score_dist[i][j]\n",
        "\n",
        "    for i in score_3main.keys():\n",
        "        value=0\n",
        "        for j in  score_3main[i].keys():\n",
        "            value+=score_3main[i][j]\n",
        "        score_3main[i]['Score']=value\n",
        "    total_features_10=[30,30,30,30,30,30,10]\n",
        "    domain_score_100={}\n",
        "\n",
        "    for i,j in zip(score_3main.keys(),total_features_10):\n",
        "        value=0\n",
        "        value=(score_3main[i]['Score']/j)*100\n",
        "        domain_score_100[i]=abs(value)\n",
        "    arr=np.array([])\n",
        "    domain_keys = sorted(domain_score_100, key=domain_score_100.get, reverse=True)[:7]\n",
        "    for i in domain_keys:\n",
        "        arr=np.append(arr,domain_score_100[i])\n",
        "    final_score=arr.mean()\n",
        "    return abs(final_score), domain_score_100,score_3main\n",
        "a,b,c=scoring_function(\"your_zipcode\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9jdFlJIX2cS"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XW-Ed3zRYYmi",
        "I0M5jXdiYhQz",
        "ChhX46VbP8Vt",
        "Dyc1SDTX466l",
        "58nafG5jOOGL",
        "pfKAAJtvc44z",
        "HjbmCeS8dBOq",
        "ZLiA1X_-dGdm",
        "XDk31cFTgBwh",
        "u9qju7LZTaVP",
        "go1QNrYaWy0Q",
        "shSk1HqMW-Y1",
        "gKkuGSb_XwSZ"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28ad7bfa99c9473d8ad75edf3ab6696f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3721d6ee39154015a9d41c77fa40ccad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40aa9f3bdad341139ef4d0fe7eba187b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "448a3c9504244000b4c05b8912d4265e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "595a860b512140b48dd14c4764ebdcc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28ad7bfa99c9473d8ad75edf3ab6696f",
            "placeholder": "",
            "style": "IPY_MODEL_ae67d2a3505d46eeae274bbc95c76645",
            "value": "2/2[00:08&lt;00:00,4.06s/it]"
          }
        },
        "707e88edb4fa45018c3fe9fcddc92112": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3721d6ee39154015a9d41c77fa40ccad",
            "placeholder": "",
            "style": "IPY_MODEL_448a3c9504244000b4c05b8912d4265e",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "8c7cbd57dc814c72b775745aba71b6e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2758bf6e0724c6b8c489feb0bc36058": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae67d2a3505d46eeae274bbc95c76645": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b27a5cdffbfc4878bf44dc4619b1180d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40aa9f3bdad341139ef4d0fe7eba187b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c7cbd57dc814c72b775745aba71b6e9",
            "value": 2
          }
        },
        "dd9aaf8d6dc64d25a459d34350881b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_707e88edb4fa45018c3fe9fcddc92112",
              "IPY_MODEL_b27a5cdffbfc4878bf44dc4619b1180d",
              "IPY_MODEL_595a860b512140b48dd14c4764ebdcc2"
            ],
            "layout": "IPY_MODEL_a2758bf6e0724c6b8c489feb0bc36058"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
